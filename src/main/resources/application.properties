spring.application.name=spring-ai-ollama-demo
# default ollama localhost url:port
spring.ai.ollama.base-url=http://localhost:11434
# choose a model (each one has built for specific purposes)
spring.ai.ollama.chat.model=gemma3:1b
#spring.ai.ollama.chat.model=deepseek-r1:1.5b
#spring.ai.ollama.chat.model=llama3.2:3b
#spring.ai.ollama.chat.model=llava:7b
#spring.ai.ollama.chat.model=codegemma:2b
spring.ai.ollama.chat.options.temperature=0.8